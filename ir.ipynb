{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "file_path = 'text_doc.txt'\n",
    "with open(file_path) as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(token) for token in filtered_words]\n",
    "stemmed_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Load data\n",
    "columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
    "           'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", \n",
    "                   names=columns)\n",
    "\n",
    "# Basic preprocessing\n",
    "data = data.replace('?', np.nan).dropna()\n",
    "numeric_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "for col in numeric_cols:\n",
    "    data[col] = pd.to_numeric(data[col])\n",
    "\n",
    "# Simple binary features\n",
    "data['age_high'] = (data['age'] > 60).astype(int)\n",
    "data['bp_high'] = (data['trestbps'] > 140).astype(int)\n",
    "data['chol_high'] = (data['chol'] > 240).astype(int)\n",
    "data['target'] = (data['target'] > 0).astype(int)\n",
    "\n",
    "# Select final features\n",
    "features = ['age_high', 'bp_high', 'chol_high', 'cp', 'exang', 'target']\n",
    "data = data[features].astype(int)\n",
    "\n",
    "# Create and train model\n",
    "model = BayesianNetwork([\n",
    "    ('age_high', 'target'),\n",
    "    ('bp_high', 'target'),\n",
    "    ('chol_high', 'target'),\n",
    "    ('cp', 'target'),\n",
    "    ('exang', 'target')\n",
    "])\n",
    "\n",
    "model.fit(data, estimator=MaximumLikelihoodEstimator)\n",
    "inference = VariableElimination(model)\n",
    "\n",
    "# Make prediction\n",
    "evidence = {\n",
    "    'age_high': 0,    # age > 60\n",
    "    'bp_high': 1,     # blood pressure > 140\n",
    "    'chol_high': 1,   # cholesterol > 240\n",
    "    'cp': 1 ,          # chest pain type\n",
    "    'exang': 0       # exercise induced angina\n",
    "}\n",
    "\n",
    "result = inference.query(variables=['target'], evidence=evidence)\n",
    "print(f\"Probability of Heart Disease: {result.values[1]:.2%}\")\n",
    "\n",
    "# Print data distribution\n",
    "print(\"\\nData Distribution:\")\n",
    "for col in features:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(data[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram,linkage\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled[:2]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_pca[:2]\n",
    "\n",
    "\n",
    "Z = linkage(X_pca,'ward')\n",
    "plt.figure(figsize=(10,6))\n",
    "dendrogram(Z)\n",
    "plt.show()\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=3)\n",
    "clustering.fit(X_pca)\n",
    "\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1],c=clustering.labels_,cmap='rainbow')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_links(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return {urljoin(url, a['href']) for a in soup.find_all('a', href=True) \n",
    "                if urljoin(url, a['href']).startswith(url)}\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "# Websites to analyze\n",
    "websites = [\n",
    "    'https://www.ted.com',\n",
    "    'https://www.goodreads.com',\n",
    "    'https://www.airbnb.com',\n",
    "    'https://www.khanacademy.org'\n",
    "]\n",
    "\n",
    "# Get links for each website\n",
    "link_map = {url: get_links(url) for url in websites}\n",
    "\n",
    "# Calculate PageRank\n",
    "d = 0.85  # damping factor\n",
    "pagerank = {url: 1/len(websites) for url in websites}\n",
    "\n",
    "# Run PageRank algorithm for 20 iterations\n",
    "for _ in range(20):\n",
    "    new_rank = {}\n",
    "    for page in websites:\n",
    "        # Calculate incoming PageRank\n",
    "        incoming_pr = sum(pagerank[src] / len(links) \n",
    "                         for src, links in link_map.items() \n",
    "                         if page in links and links)\n",
    "        # Update PageRank\n",
    "        new_rank[page] = (1 - d) / len(websites) + d * incoming_pr\n",
    "    pagerank = new_rank\n",
    "\n",
    "# Normalize scores\n",
    "total = sum(pagerank.values())\n",
    "pagerank = {url: score/total for url, score in pagerank.items()}\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "sorted_pr = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n",
    "sites, scores = zip(*sorted_pr)\n",
    "plt.barh([site.replace('https://www.', '').replace('.com', '').replace('.org', '') \n",
    "          for site in sites], scores)\n",
    "plt.xlabel('PageRank Score')\n",
    "plt.title('Website PageRank Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print scores\n",
    "for url, score in sorted_pr:\n",
    "    print(f'{url}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL of the e-commerce site with pagination structure\n",
    "base_url = 'http://books.toscrape.com/catalogue/page-{}.html'\n",
    "\n",
    "# Function to scrape product data from a single page\n",
    "def scrape_books(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    books = []\n",
    "    for product in soup.select('article.product_pod'):\n",
    "        title = product.h3.a['title']\n",
    "        price = product.select_one('.price_color').get_text(strip=True)\n",
    "        link = product.h3.a['href']\n",
    "        full_link = requests.compat.urljoin(url, link)  # Join relative link with base URL\n",
    "\n",
    "        books.append({\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'link': full_link\n",
    "        })\n",
    "    return books\n",
    "\n",
    "# Loop through multiple pages\n",
    "all_books = []\n",
    "for page_num in range(1, 6):  # Adjust the range based on the number of pages\n",
    "    url = base_url.format(page_num)\n",
    "    try:\n",
    "        books_data = scrape_books(url)\n",
    "        if not books_data:  # Stop if no books are found on the page (end of pagination)\n",
    "            break\n",
    "        all_books.extend(books_data)\n",
    "        print(f\"Page {page_num} scraped successfully.\")\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"Failed to scrape page {page_num}: {e}\")\n",
    "        break\n",
    "\n",
    "# Print all book data collected\n",
    "for book in all_books:\n",
    "    print(f\"Title: {book['title']}\")\n",
    "    print(f\"Price: {book['price']}\")\n",
    "    print(f\"Link: {book['link']}\")\n",
    "    print('-' * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fasta_file) as file:\n",
    "    lines = file.readlines()\n",
    "    print(lines)\n",
    "\n",
    "sequence = ''\n",
    "\n",
    "with open(fasta_file) as file:\n",
    "    lines = file.readlines()\n",
    "    sequence = ''.join(line.strip() for line in lines[1:]).upper()\n",
    "\n",
    "\n",
    "a_count = sequence.count('A')\n",
    "t_count = sequence.count('T')\n",
    "\n",
    "total_count = len(sequence)\n",
    "\n",
    "at_percent = ((a_count + t_count) * 100) / total_count \n",
    "\n",
    "g_count = sequence.count('G')\n",
    "c_count = sequence.count('C')\n",
    "\n",
    "gc_percent = ((g_count + c_count)*100)/total_count\n",
    "\n",
    "at_percent / gc_percent\n",
    "\n",
    "start_codon = 'ATG'\n",
    "# stop_codon = 'TAA'\n",
    "stop_codons = ['TAA', 'TAG', 'TGA']\n",
    "\n",
    "start_index = sequence.find(start_codon)\n",
    "\n",
    "coding_region = []\n",
    "\n",
    "while start_index != -1:\n",
    "\n",
    "    for stop_codon in stop_codons:\n",
    "        stop_index = sequence.find(stop_codon,start_index+3)\n",
    "\n",
    "        if stop_index != -1 and (stop_index - start_index) % 3 == 0:\n",
    "            coding_seq = sequence[start_index:stop_index+3]\n",
    "            coding_region.append(coding_seq)\n",
    "            break\n",
    "\n",
    "    start_index = sequence.find(start_codon,start_index+1)\n",
    "\n",
    "\n",
    "motif = 'TATAA'\n",
    "\n",
    "start_index = sequence.find(motif)\n",
    "\n",
    "while start_index != -1:\n",
    "\n",
    "    print(f\"Motif '{motif}' found at positions {start_index}\")\n",
    "\n",
    "    start_index = sequence.find(motif,start_index+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
